{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_csv_export_dataset(\"../export_dataframe_stage2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data to concentrate on recent period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"event_date\"] > \"2012-01-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core fields are air_temp, water_temp, ph1, ph2, DissolvedOxygen1, DissolvedOxygen2 and Conductivity. For those who has two values, we could combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMean(v1, v2, populate_NA = True):\n",
    "    if pd.isnull(v1) and pd.notnull(v2) and populate_NA:\n",
    "        return v2\n",
    "    elif pd.notnull(v1) and pd.isnull(v2) and populate_NA:\n",
    "        return v1\n",
    "    elif pd.notnull(v1) and pd.notnull(v2):\n",
    "        return (v1+v2)/2\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df[\"mean_ph\"] = df.apply(lambda row: getMean(row[\"ph1\"], row[\"ph2\"]), axis=1)\n",
    "df[\"mean_DissolvedOxygen\"] = df.apply(lambda row: getMean(row[\"DissolvedOxygen1\"], row[\"DissolvedOxygen2\"]), axis=1)\n",
    "df.drop(columns=[\"ph1\", \"ph2\", \"DissolvedOxygen1\", \"DissolvedOxygen2\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some semi-core fields: SecchiDisk1, SecchiDisk2, ChlorophyIIA, Salinity1, Salinity2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mean_SecciDisk\"] = df.apply(lambda row: getMean(row[\"SecchiDisk1\"], row[\"SecchiDisk2\"]), axis=1)\n",
    "df[\"mean_Salinity\"] = df.apply(lambda row: getMean(row[\"Salinity1\"], row[\"Salinity2\"]), axis=1)\n",
    "df.drop(columns=[\"SecchiDisk1\", \"SecchiDisk2\", \"Salinity1\", \"Salinity2\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also tow fields represents the same data: air_temp and air_temperature.  \n",
    "Merge them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['air_temp'] = df.apply(lambda row: getMean(row[\"air_temp\"], row[\"air_temperature\"]), axis=1)\n",
    "df.drop(columns=[\"air_temperature\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I propose a strategy to categorize data into 3 groups: core fields, semi-core fields and other fields. To measure the quality of an entry, we add three columns to count the number of missing fields in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the groups\n",
    "core_group = [\"air_temp\", \"water_temp\", \"mean_ph\", \"mean_DissolvedOxygen\", \"Conductivity\"]\n",
    "semi_core_group = [\"mean_SecciDisk\", \"mean_Salinity\", \"ChlorophyllA\"]\n",
    "others = list(set(df)-set(core_group)-set(semi_core_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_count(row, group):\n",
    "    return sum([pd.isnull(row[col]) for col in group])\n",
    "\n",
    "df[\"core_missing_count\"] = df.apply(lambda row: get_missing_count(row, core_group), axis=1)\n",
    "df[\"semi_core_missing_count\"] = df.apply(lambda row: get_missing_count(row, semi_core_group), axis=1)\n",
    "df[\"others_missing_count\"] = df.apply(lambda row: get_missing_count(row, others), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we got a count of missing fields, we could aggregate the count on the site level (wbd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cols = [\"core_missing_count\", \"semi_core_missing_count\", \"others_missing_count\"]\n",
    "missing_count = df.groupby(['wbd'])[count_cols].sum().sort_values(by=count_cols, ascending=True)\n",
    "missing_count.reset_index(inplace=True)\n",
    "missing_count.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, only counting on missing fields doesn't tell us about how it's distributed in the time. We need to ensure that we have at least 1 entries for each site every 6 months to show the trend."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 496 sites in total. We first select a cut off solely base on the missing count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count = missing_count.query(\"core_missing_count < 20 and semi_core_missing_count < 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to aggregate the original data again to find the sites that have at least 1 entry every 6 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = df.groupby(['wbd',pd.Grouper(key='event_date', freq='6M')]).count().reset_index()\n",
    "sites = sites[['wbd','event_date', 'event_rid']]\n",
    "time_span = df[\"event_date\"].max() - df[\"event_date\"].min()\n",
    "time = 2*time_span.days/365\n",
    "sites = sites.query(\"event_rid >= 1\")\n",
    "sites = sites.groupby(['wbd']).count().reset_index()\n",
    "sites = sites.query(\"event_rid >= @time\")\n",
    "site_names = sites['wbd'].tolist()\n",
    "site_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these sites on map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely.geometry import shape, Point\n",
    "# pip install pyshp\n",
    "import shapefile\n",
    "# pip install geopy\n",
    "from geopy import distance\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "import plotly as plt\n",
    "pio.renderers.default = \"jupyterlab\"\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = df[['SiteName', 'SiteLocation']]\n",
    "df = df[~df['SiteLocation'].isna()]\n",
    "locs = locs[~locs['SiteLocation'].isna()]\n",
    "loc_pairs, geo_locs, gdf = get_loc_objects_from_series(locs['SiteLocation'])\n",
    "loc_lookup = fetch_geo_locs()\n",
    "WBD_gj = shapefile.Reader(\"../../geodata\\hydrologic_units_WBDHU12_ga_3975106_02\\hydrologic_units\\wbdhu12_a_ga.shp\").__geo_interface__\n",
    "GA_map = get_state_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth_mapbox(sites, #array/object with wbd names in the same order as the json file\n",
    "                           geojson=WBD_gj,\n",
    "                           locations=\"wbd\",\n",
    "                           featureidkey=\"properties.name\",\n",
    "                           center={\"lat\": 32.8407, \"lon\": -83.6324}, # macon\n",
    "                           mapbox_style=\"stamen-terrain\", #\"carto-positron\",\n",
    "                           color_continuous_scale=\"Jet\", #Sunsetdark, Jet, \n",
    "                           zoom=6,\n",
    "                           opacity=0.7,\n",
    "                      )\n",
    "\n",
    "\n",
    "fig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sci4ga')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2148c183b74d5232b0a65c75cd4f040fca81da9bc0b6e87013a035d3dfe8209f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
